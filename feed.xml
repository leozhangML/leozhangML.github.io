<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://leozhangml.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://leozhangml.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-10T21:13:50+00:00</updated><id>https://leozhangml.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">an introduction to discrete diffusion and time-inhomogeneous CTMCs</title><link href="https://leozhangml.github.io/blog/2025/intro-to-discrete-diffusion/" rel="alternate" type="text/html" title="an introduction to discrete diffusion and time-inhomogeneous CTMCs"/><published>2025-08-03T16:40:16+00:00</published><updated>2025-08-03T16:40:16+00:00</updated><id>https://leozhangml.github.io/blog/2025/intro-to-discrete-diffusion</id><content type="html" xml:base="https://leozhangml.github.io/blog/2025/intro-to-discrete-diffusion/"><![CDATA[<p><strong>See the notes <a href="/assets/pdf/Introduction_to_Discrete_Diffusion-1.pdf">here</a></strong></p> <p>I’ve recently been working with discrete diffusion models and from digging into the literature around this topic, I’ve been annoyed at just how hard it is to find a good, unified presentation of the main theory behind discrete diffusion, namely time-inhomogeneous continuous-time Markov chains (CTMCs).</p> <p>The issue arises from the fact that most mathematical presentations of CTMCs only work with the time-homogeneous case. Hence, finding an explanation of the time-dependent rate matrix, and how this gives rise to important results like the Kolmogorov forward and backward equations (and more generally, the theory of time-inhomogeneous Markov processes described by evolution systems) is a frustrating process.</p> <p>To help others skip this time-consuming journey, I’ve collected the main results, theory and references which I’ve found helpful to learn about time-inhomogeneous CTMCs in these slides which can be found <a href="/assets/pdf/Introduction_to_Discrete_Diffusion-1.pdf">here</a>. I also provide an introduction to the main papers on discrete diffusion models and discuss how they connect to the underlying theory. I hope these notes can be useful to the wider machine learning community.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[or, why does no one talk about time-inhomogeneous Markov processes?]]></summary></entry><entry><title type="html">scaling up bayesian nonparametric manifold learning</title><link href="https://leozhangml.github.io/blog/2023/manifold_vi/" rel="alternate" type="text/html" title="scaling up bayesian nonparametric manifold learning"/><published>2023-11-26T16:40:16+00:00</published><updated>2023-11-26T16:40:16+00:00</updated><id>https://leozhangml.github.io/blog/2023/manifold_vi</id><content type="html" xml:base="https://leozhangml.github.io/blog/2023/manifold_vi/"><![CDATA[<p>This is an excerpt from my master’s thesis, for which the code can be found <a href="https://github.com/leozhangML/manifold_vi">here</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/spiral-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/spiral-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/spiral-1400.webp"/> <img src="/assets/img/spiral.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: (Left) Plot of the 3D spiral (1,000 points with $\delta=0.1$) dataset from Berenfeld et al. [2022]. (Right) Plot of 1,000 points sampled from the posterior predictive distribution given by our learned variational approximation. </div> <p>The manifold hypothesis states that naturally generated, high-dimensional data, such as images or neural activity, should be expected to lie around some low-dimensional manifold. This has been justified by the fact that natural data is generated under physical constraints which limit its intrinsic complexity. The success of machine learning methods, such as neural networks, on high-dimensional data can thus be explained by their ability to exploit this latent, low-dimensional structure.</p> <p>The field of manifold learning (Bengio et al. [2014]) then is concerned with leveraging the intuition behind the manifold hypothesis to design machine learning algorithms, which can represent these geometric structures. For example, the algorithms: Isomap (Tenenbaum et al. [2000]) and Umap (McInnes et al. [2018]), apply geometric concepts to provide dimensionality reduction for data visualisation purposes.</p> <p>Another area of manifold learning is density estimation. While density estimation is an established problem within statistics, being useful in downstream clustering or prediction tasks, the manifold hypothesis suggests a natural framework for estimation. This has seen particular interest from research in deep generative models (Tomczak [2022]) due the fact that such models aim to learn the distribution of our data to generate new, interesting samples. By accounting for the geometric structure of data, such models should be able to learn better data distributions for improved performance. Brown et al. [2023] presents evidence in this direction, showing that inductive biases within deep generative models, which account for the manifold hypothesis, can help improve model performance. Also, Horvat and Pfister [2023] adapts normalising flows to learn distributions supported on low-dimensional submanifolds.</p> <p>In this dissertation, however, we are interested in the Bayesian nonparametric approach to density estimation. The standard way this is done is through Dirichlet process mixtures - this specifies our density by an infinite mixture model, with a Dirichlet process prior placed on the mixture’s parameters. Bayesian inference then provides an estimate of the true density. While such models have long been studied in the literature (Neal [2000]), they mainly assume that the distribution of our data is supported on \(\mathbb{R}^D\) instead of possessing some low-dimensional structure. Hence, these approaches have been shown to struggle in recovering the density of manifold-distributed data as they are unable to capture the curvature and (low) dimensionality of such structures - see Mukhopadhyay et al.[2020].</p> <p>The recent paper Berenfeld et al. [2022] overcomes these limitation through introducing a new family of Dirichlet process mixtures, for which they show very good empirical performance. Theoretical guarantees are also provided to show that such mixtures are able to converge to the “true” distribution of the data, under regularity conditions. Along with the interpretability of such methods compared to black-box deep generative models, this provides a compelling solution to density estimation under the manifold hypothesis. The main drawback, however, is that inference - through MCMC sampling - is computationally expensive and fails to scale to datasets with dimension greater than two. This prevents the use of such methods beyond toy examples.</p> <p>This motivates the main contributions of this dissertation. We present a variational inference algorithm to provide approximate Bayesian inference for Berenfeld et al. [2022] - see Figure 1. This allows us to scale such methods to higher-dimensional datasets, for which we empirically observe only a slight drop in accuracy. We also attempt to give theoretical guarantees for the convergence rate of an oracle variational approximation to the true parameters, in order to provide justification for the use of variational inference. We note that while we were unable to finish the proof before the dissertation deadline, our proof only requires one final technical step.</p> <h3 id="references">References:</h3> <p>Bengio, Y., Courville, A. and Vincent, P., 2013. Representation learning: A review and new perspectives. <em>IEEE transactions on pattern analysis and machine intelligence, 35</em>(8), pp.1798-1828.</p> <p>Berenfeld, C., Rosa, P. and Rousseau, J., 2022. Estimating a density near an unknown manifold: a Bayesian nonparametric approach. <em>arXiv preprint arXiv:2205.15717</em>.</p> <p>Brown, B.C., Caterini, A.L., Ross, B.L., Cresswell, J.C. and Loaiza-Ganem, G., 2022. Verifying the union of manifolds hypothesis for image data. In <em>The Eleventh International Conference on Learning Representations</em>.</p> <p>Horvat, C. and Pfister, J.P., 2023. Density estimation on low-dimensional manifolds: an inflation-deflation approach. <em>J. Mach. Learn. Res., 24</em>, pp.61-1.</p> <p>McInnes, L., Healy, J. and Melville, J., 2018. Umap: Uniform manifold approximation and projection for dimension reduction. <em>arXiv preprint arXiv:1802.03426</em>.</p> <p>Mukhopadhyay, M., Li, D. and Dunson, D.B., 2020. Estimating densities with non-linear support by using Fisher–Gaussian kernels. <em>Journal of the Royal Statistical Society Series B: Statistical Methodology, 82</em>(5), pp.1249-1271.</p> <p>Neal, R.M., 2000. Markov chain sampling methods for Dirichlet process mixture models. <em>Journal of computational and graphical statistics, 9</em>(2), pp.249-265.</p> <p>Tenenbaum, J.B., Silva, V.D. and Langford, J.C., 2000. A global geometric framework for nonlinear dimensionality reduction. <em>science</em>, 290(5500), pp.2319-2323.</p> <p>Tomczak, J.M., 2022. Deep Generative Modeling. <em>Springer</em>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[an introduction to my master's thesis]]></summary></entry><entry><title type="html">how to rent a room</title><link href="https://leozhangml.github.io/blog/1996/howto/" rel="alternate" type="text/html" title="how to rent a room"/><published>1996-10-01T00:00:00+00:00</published><updated>1996-10-01T00:00:00+00:00</updated><id>https://leozhangml.github.io/blog/1996/howto</id><content type="html" xml:base="https://leozhangml.github.io/blog/1996/howto/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/K8Jjosf5GrE" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[march & april, looking forward to summer]]></summary></entry></feed>